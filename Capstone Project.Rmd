---
title: "Data Science Specialization Capstone Project"
author: "Alejandro Osorio"
date: "September 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set
library(tidyverse)
library(stringr)
library(cld3)
library(caret)
set.seed(123)
```

## Executive Summary

### Getting and Cleaning Data

The News dataset required cleaning in order to obtain 100% of the dataset.  The rest of the datasets (Twitter and Blogs) were obtained without cleaning required (details of the reading process, in Appendix I.a).  Details of the cleaning process required for the News dataset, in Appendix I.b.

Finally, the preliminary datasets containing separate-underscore-words were generated in Appendix I.c.

### Preliminary Analysis

Based on a word-frequency analysis, it's worth noting that words such as "the", "to", "a", "and" were the most frequent ones throughout all datasets.  Details of the quantitative and visual results, can be seen in Appendix II.b.

Additionally, some preliminary n-grams functions, that identify all two-grams and three-grams for each dataset row, were successfully programmed.  Details of said process in Appendix II.a.

## I. Appendix 1: Getting and Cleaning Data

### I.a. Reading Data

#### Twitter Dataset
```{r, cache = TRUE}
con <- file("en_US.twitter.txt", "r")
datatwitter <- readLines(con)
close(con)
```

Given the warning, the dimensions of both the obtained tibble and the original file, were compared.  They were equal.  Additionally, after some basic visual comparisons (head, tail, str, is.na), it was concluded that the obtained tibble was ok to work with.

#### Blogs Dataset
```{r, cache = TRUE}
con <- file("en_US.blogs.txt", "r")
datablogs <- readLines(con)
close(con)
```

After the same basic visual comparisons (head, tail, str, is.na) used with the previous dataset, it was concluded that the obtained tibble was also ok to work with.

#### News Dataset
```{r, cache = TRUE}
con <- file("en_US.news.txt", "r")
datanews_err <- readLines(con)
close(con)
```

Given the warning, the dimensions of both the obtained tibble and the original file, were compared.  The total number of lines obtained with the tibble (77,259) were much less than the ones from the original txt file (1,010,242).  Therefore, some cleaning was required, as follows.

### I.b. Cleaning Original Data

#### Cleaning News dataset

Further analysis to the "news" dataset, showed that line 77,259 was cut at the character "1", when compared to the one from the original file ("Ultra Edit" text editor was used for reading the original dataset, as well as the next cleaning steps). Therefore line 77,259 was analysed and an EOF character was found between texts "1" and "gallons", within the phrase "producing 1 -EOF- gallons".  Said character was deleted and the process re-run.  The result was a tibble with 766,277 rows (still less than the original file).  Another EOF character was found in row 766.277 (between texts "in" and "- inch", within the phrase "pears in EOF - inch slices."), deleted and the process re-run. Again, fewer rows were obtained (926,143).  Yet another EOF character was found in row 926,143 (between texts "in" and "-inch" within the phrase "Slice in EOF -inch-thick pieces"), deleted and the process re-run. Still, but last, fewer rows were obtained (948,564).  EOF character was found in row 926,143 (between texts "1" and "pounds" within the phrase "call for 1 EOF pounds of tomatoes"), deleted and the process re-run again.  Finally, 1,010,242 lines were obtained from the final fixed file, as follows:

```{r, cache = TRUE}
con <- file("en_US.news_FINAL.txt", "r")
datanews <- readLines(con)
close(con)
rm(con)
glimpse(datanews)
```

With this result, plus the same basic visual comparisons (head, tail, str, is.na) used with the previous datasets, it was concluded that the obtained tibble was also ok to work with and therefore the cleaning stage was considered finished.

#### Cleaning foreign languages/ damaged strings

Some rows show words weren't useful for predictive modeling, such as:
```{r}
datablogs[19]
```

Eventhough it's english written, the extra work needed to clean such cases was considered unnecesary.  Instead, a language detector (library "cld3") was used to filter both such cases and rows containing foreign languages.  As an example:
```{r}
detect_language(datablogs[20])
```

Regardless of the result ("fr" stands for french), it was consedered useful for the purpose of eliminating rows containing unclean or foreign text.

Therefore, all three datasets were filtered from rows other than "en", using the following function:
```{r, cache = TRUE}
langfilt <- function(lng, dataset) {
        lngvect <- lng == detect_language(dataset) %>%
                        replace_na(., FALSE)
        dataset <- dataset[lngvect == TRUE]
        return(dataset)
}
```

Cleaning datasets:
```{r, cache = TRUE}
strings_blogs <- langfilt(lng = "en", dataset = datablogs)
strings_news <- langfilt(lng = "en", dataset = datanews)
strings_twitter <- langfilt(lng = "en", dataset = datatwitter)
```

Finally, with each obtained set, an extra set of separate underscore-words was created, using the following function that converts a vector of character strings into a vector with the strings turned into lower-case and space-separated words
```{r}
lowcasewords <- function(dataset) {
        datalength <- length(dataset)
        result <- dataset %>%
                str_to_lower(.) %>%
                str_extract_all(., boundary("word"))
        for(i in 1:datalength) {
                result[[i]] <- str_flatten(result[[i]], collapse = " ")
        }
        return(unlist(result))
}
```

Applying the function to create the final datasets:
```{r, cache = TRUE}
words_twitter <- lowcasewords(strings_twitter)
words_blogs <- lowcasewords(strings_blogs)
words_news <- lowcasewords(strings_news)
```

An example of the separate-underscore-english-words results obtained:

```{r, echo = FALSE}
head(words_news, 3)
```


## II. Appendix 2: Preliminary Analysis

### II.a. Basic Analysis Functions

#### Single row n-grams generator

Function that inputs a vector of character strings and, with one specific character string, returns another character vector with all of its possible n-grams.

```{r}
ngramrow <- function(ngram, dataset, rownum) {
        ngrams <- character(length = 0)
        lengthstring <- str_count(dataset[rownum], boundary("word"))
        if(lengthstring < ngram) {
                ngrams <- NA
        } else {
                qngrams <- lengthstring+1-ngram
                for(j in 1:qngrams) {
                                ngrams <- c(ngrams, word(dataset[rownum], j, ngram + j - 1))
                                }
                }
        return(unique(ngrams))
        }
```

As an example follows a 3-gram generation, using the 3rd row of the news data:

3-gram:
```{r, echo = FALSE}
head(ngramrow(ngram = 3, dataset = words_news, rownum = 3), 5)
```

#### Single row n-grams frequency counter

Function that nested the previous function, within a loop along the length of the selected vector's string.  The returned data frame was grouped by the corresponding n-grams, summarized and arranged by frequency in order to easily visualize the most frequent ones in the given row.
```{r}
ngramrowfreq <- function(ngram, dataset, rownum) {
        stringrow <- dataset[rownum]
        uniquengramsrow <- ngramrow(ngram, dataset, rownum)
        numngrams <- length(uniquengramsrow)
        rowfreq <- data_frame(string = character(), freq = numeric())
        for(i in 1:numngrams) {
                uniquengram <- paste("\\b", uniquengramsrow[i], "\\b", sep = "")
                uniquengramfreq <- str_count(stringrow, uniquengram)
                rowfreq[i,1] <- uniquengramsrow[i]
                rowfreq[i,2] <- uniquengramfreq
        }
        return(rowfreq)
}
```

As an example follows a 3-gram row frequency count, using the 3rd row of the news data:
```{r, cache = TRUE, echo = FALSE}
head(ngramrowfreq(ngram = 3, dataset = words_news, rownum= 3), 3)
```

#### Multirow n-grams frequency counter and probability calculator

Function that nested the previous function, within a loop along the length of the dataset.  The returned data frame was grouped by n-gram, summarized and arranged by frequency.  Additionally, it added a column with the dimension of each n-gram.

```{r}
ngramsetfreq <- function(ngram, dataset) {
        setfreq <- data_frame(string = character(), freq = numeric())
        for(i in 1:length(dataset)) {
               setfreq <- bind_rows(setfreq, ngramrowfreq(ngram, dataset, rownum = i))
        }
        setfreq <- setfreq %>%
                group_by(string) %>%
                summarize (., freq = sum(freq)) %>%
                arrange(., desc(freq)) %>%
                mutate(., n = ngram)
        return(setfreq)
}
```

As an example follows a 3-gram frequency count, using the first 5 rows of the news dataset:
```{r, cache = TRUE, echo = FALSE}
head(ngramsetfreq(ngram = 3, dataset = words_news[1:5]), 4)
```

### II.b. Results Obtained by Dataset

#### Twitter dataset

Applying the Multirow n-grams frequency counter to the first 1,000 rows of the dataset, the following results were obtained for 1 and 3-grams:

1-grams:
```{r, echo = FALSE, cache = TRUE}
freqword_twitter <- ngramsetfreq(ngram = 1, dataset = words_twitter[1:1000])
barplot(names.arg = head(freqword_twitter)$string, height = head(freqword_twitter)$freq)
```

3-grams:
```{r, echo = FALSE, cache = TRUE}
freqthreegrams_twitter <- ngramsetfreq(ngram = 3, dataset = words_twitter[1:1000])
barplot(names.arg = head(freqthreegrams_twitter)$string, height = head(freqthreegrams_twitter)$freq)
```

#### Blogs dataset

Likewise, the following 3-gram results were obtained.
```{r, echo = FALSE, cache = TRUE}
freqthreegrams_blogs <- ngramsetfreq(ngram = 3, dataset = words_blogs[1:1000])
barplot(names.arg = head(freqthreegrams_blogs)$string, height = head(freqthreegrams_blogs)$freq)
```

#### News dataset

Finally, the following results were obtained for the news dataset.
```{r, echo = FALSE, cache = TRUE}
freqthreegrams_news <- ngramsetfreq(ngram = 3, dataset = words_news[1:1000])
barplot(names.arg = head(freqthreegrams_news)$string, height = head(freqthreegrams_news)$freq)
```

## III. Appendix 3: Predictive Models

### III.a. Preliminary Training Sets

#### Primary Set

The training set was built by mixing a sample of each one of the three datasets, as follows:
```{r, cache = TRUE}
sampletrain <- c(sample(words_blogs, 1000), sample(words_news, 1000), sample(words_twitter, 1000))
```

#### n-gram Sets

Function that generates n-grams (from 2-grams up to the n specified) based on the Primary Set (previously obtained). 
```{r}
ngramsetgen <- function(n, dataset) {
        ngramsets <- data_frame(string = character(), freq = integer(), n = integer())
        for(i in 2:n) {
                ngramset <- ngramsetfreq(ngram = i, dataset = dataset)
                ngramsets <- bind_rows(ngramsets, ngramset)
        }
        return(ngramsets)
}
```

The following set was pre-calculated, in order to run the predictive algorithm as an n-gram search machine.

```{r, cache = TRUE}
samplengram <- ngramsetgen(5, sampletrain)
```

### III.b. Predictive Algorithms Based on Searching Pre-calculated n-grams

#### Function that, given a string, filters from the sample ngram dataset the n-grams from which to predict the next word

In case there is no n-gram available in the samplengram dataset, it reduces n by 1 (and the string in 1 word) and repeats until it finds the best available n-gram set.
```{r}
ngrampred <- function(string, dataset) {
        numwords <- str_count(string, boundary("word"))
        disp <- string %in% dataset[[1]]
        while(disp == FALSE) {
                numwords <- numwords - 1
                string <- word(string, 2, -1)
                disp <- string %in% dataset[[1]]
        }
        stringsearch <- paste("^", string, "\\b", sep = "")
        ngramset <- dataset %>%
                filter(., n == (numwords + 1) & str_detect(string, stringsearch))
        return(ngramset)
}
```

As an example, if applied with the 2-gram "on the":
```{r}
head(ngrampred(string = "on the", dataset = samplengram), 3)
```

Likewise, if applied with the 4-gram "and a case of":
```{r}
head(ngrampred(string = "and a case of", dataset = samplengram), 3)
```
It can be seen that, as there were no 5-grams including the string "and a case of <word>", the function then reduced the search string to "a case of" and then searched for a 4-gram that included said string ("a case of <word>"), and so forth until it found the solutions.

#### Function that selects the predicted word, from previous function's output

In case several rows have the same maximum frequency, a random row within that subset was chosen, as follows:
```{r}
wordpred <- function(string, dataset) {
        word <- ngrampred(string, dataset) %>%
                filter(., freq == max(freq)) %>%
                sample_n(., 1) %>%
                select(., string) %>%
                word(., -1)
        return(word)
}
```

As an example, if applied again with the 4-gram string "and a case of":
```{r}
wordpred(string = "and a case of", dataset = samplengram)
```

The problem with the "search engine" approach, is that it requires enormous amount of processing in order to generate all n-gram Sets from which to predict later.  Therefore a different approach was attempted further on, consisting on generating the n-gram sets based on the string that requires prediction, as follows in the next chapter.

### III.c. Predictive Algorithms Based on Searching the String with which to calculate the required n-grams

#### Function that extracts specified string, plus its next word, from a row
```{r}
stringrow <- function(string, dataset, numrow) {
        matches <- str_extract_all(dataset[numrow], paste("(^|\\s)", string, "\\s\\w+", sep = ""))[[1]]
        return(matches)
}
```

As an example, if applied with the string "plans" to the 3rd row of the news dataset:
```{r}
stringrow("plans", words_news, numrow = 3)
```

#### Function that nests previous function and applies it to a dataset
```{r}
stringset <- function(string, dataset) {
        datlength <- length(dataset)
        matches <- c()
        for(i in 1:datlength) {
                stringmatch <- stringrow(string, dataset, numrow = i)
                matches <- c(matches, stringmatch)
        }
        matches <- data_frame(string = matches) %>%
                mutate(., freq = 1) %>%
                group_by(string) %>%
                summarize (., freq = sum(freq)) %>%
                arrange(., desc(freq))
        return(matches)
}
```

As an example, if applied with the string "to the" to the first 1.000 news dataset:
```{r}
head(stringset("to the", words_news[1:1000]), 5)
```

The performance obtained with the later functions, was extremly better than the previous one.  Much so that the whole news dataset took less than 40 seconds to complete.

#### Wrapper that applies stringset function only to strings available in the input set

It starts by converting the input string to lower_case.  In case there's no match for the string in the dataset, it reduces the string's n-gram by 1 (by taking out its first word) and repeats until it finds the first available string.
```{r}
stringsure <- function(string, dataset) {
        strsearch <- str_to_lower(string)
        strwhere <- paste("(^|\\s)", strsearch, "\\s\\w+", sep = "") %>%
                str_which(dataset, .)
        strdisp <- sum(strwhere)
        while(strdisp == 0) {
                strsearch <- word(strsearch, 2, -1)
                strwhere <- paste("(^|\\s)", strsearch, "\\s\\w+", sep = "") %>%
                        str_which(dataset, .)
                strdisp <- sum(strwhere)
        }
        result <- stringset(strsearch, dataset[strwhere])
        return(result)
}
```

As an example, if applied with the string "the faith during the" on the news dataset:
```{r}
head(stringsure(string = "the faith during the", dataset = words_news), 5)
```

RESOLVER REGULAR EXPRESSION AL INICIO DE LA PALABRA BUSCADA (CON V/S SIN ESPACIO EN BLANCO => UNA SOLA SOLUCION).
VARIABILIZAR ESPACIO MUESTRAL EN FUNCIÓN DE N-GRAMAS A BUSCAR (A MAYOR N, MAYOR ESPACIO MUESTRAL)


