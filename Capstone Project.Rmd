---
title: "Data Science Specialization Capstone Project"
author: "Alejandro Osorio"
date: "September 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set
library(tidyverse)
library(stringr)
```

## Executive Summary

### Getting and Cleaning Data

The News dataset required cleaning in order to obtain 100% of the dataset.  The rest of the datasets (Twitter and Blogs) were obtained without cleaning required (details of the reading process, in Appendix I.a).  Details of the cleaning process required for the News dataset, in Appendix I.b.

Finally, the preliminary datasets containing separate-underscore-words were generated in Appendix I.c.

### Preliminary Analysis

Based on a word-frequency analysis, it's worth noting that words such as "the", "to", "a", "and" were the most frequent ones throughout all datasets.  Details of the quantitative and visual results, can be seen in Appendix II.b.

Additionally, some preliminary n-grams functions, that identify all two-grams and three-grams for each dataset row, were successfully programmed.  Details of said process in Appendix II.a.

## I. Appendix 1: Getting and Cleaning Data

### I.a. Reading Data

#### Twitter Dataset
```{r, cache = TRUE}
con <- file("en_US.twitter.txt", "r")
datatwitter <- data_frame(TEXT = readLines(con))
close(con)
```

Given the warning, the dimensions of both the obtained tibble and the original file, were compared.  They were equal.  Additionally, after some basic visual comparisons (head, tail, str, is.na), it was concluded that the obtained tibble was ok to work with.

#### Blogs Dataset
```{r, cache = TRUE}
con <- file("en_US.blogs.txt", "r")
datablogs <- data_frame(TEXT = readLines(con))
close(con)
```

After the same basic visual comparisons (head, tail, str, is.na) used with the previous dataset, it was concluded that the obtained tibble was also ok to work with.

#### News Dataset
```{r, cache = TRUE}
con <- file("en_US.news.txt", "r")
datanews_err <- data_frame(TEXT = readLines(con))
close(con)
```

Given the warning, the dimensions of both the obtained tibble and the original file, were compared.  The total number of lines obtained with the tibble (77,259) were much less than the ones from the original txt file (1,010,242).  Therefore, some cleaning was required, as follows.

### I.b. Cleaning Data

Further analysis to the "news" dataset, showed that line 77,259 was cut at the character "1", when compared to the one from the original file ("Ultra Edit" text editor was used for reading the original dataset, as well as the next cleaning steps). Therefore line 77,259 was analysed and an EOF character was found between texts "1" and "gallons", within the phrase "producing 1 -EOF- gallons".  Said character was deleted and the process re-run.  The result was a tibble with 766,277 rows (still less than the original file).  Another EOF character was found in row 766.277 (between texts "in" and "- inch", within the phrase "pears in EOF - inch slices."), deleted and the process re-run. Again, fewer rows were obtained (926,143).  Yet another EOF character was found in row 926,143 (between texts "in" and "-inch" within the phrase "Slice in EOF -inch-thick pieces"), deleted and the process re-run. Still, but last, fewer rows were obtained (948,564).  EOF character was found in row 926,143 (between texts "1" and "pounds" within the phrase "call for 1 EOF pounds of tomatoes"), deleted and the process re-run again.  Finally, 1,010,242 lines were obtained from the final fixed file, as follows:

```{r, cache = TRUE}
con <- file("en_US.news_FINAL.txt", "r")
datanews <- data_frame(TEXT = readLines(con))
close(con)
glimpse(datanews)
```

With this result, plus the same basic visual comparisons (head, tail, str, is.na) used with the previous datasets, it was concluded that the obtained tibble was also ok to work with and therefore the cleaning stage was considered finished.

### I.c. Preparing Preliminary Datasets

In order to work with sets words, each line of the obtained datasets was converted into a list of separate-underscore-words, as follows:

```{r, cache = TRUE}
words_twitter <- datatwitter$TEXT %>%
        str_to_lower() %>%
        str_extract_all(., boundary("word"))
words_blogs <- datablogs$TEXT %>%
        str_to_lower() %>%
        str_extract_all(., boundary("word"))
words_news <- datanews$TEXT %>%
        str_to_lower() %>%
        str_extract_all(., boundary("word"))
```

An example of the results obtained:

```{r, echo = FALSE}
head(words_twitter, 2)
```

## II, Appendix 2: Preliminary Analysis

### II.a. Basic Analysis Functions

#### Single row word frequency counter

Function that returns a dataframe with two columns (unique word and its frequency within given row).

```{r}
wordrowfreq <- function(dataset, rownum) {
        rowfreq <- data_frame(word = character(), freq = numeric())
        uniquewordsrow <- unique(dataset[[rownum]])
        for(i in seq_along(uniquewordsrow)) {
                uniqueword <- paste("\\b", uniquewordsrow[i], "\\b", sep = "")
                uniquewordfreq <- str_count(dataset[rownum], uniqueword)
                rowfreq[i,1] <- uniquewordsrow[i]
                rowfreq[i,2] <- uniquewordfreq
        }
        return(rowfreq)
}
```

As an example follows an extract of the results obtained using 3rd row of news data:

```{r, echo = FALSE}
head(wordrowfreq(dataset = words_news, rownum = 3), 3)
```

Version 2.0 (lower case paragraph based) PENDIENTE CORREGIR #$%& UNIQUE (NO FUNCIONA)

```{r}
wordrowfreq2 <- function(dataset, rownum) {
        rowfreq <- data_frame(word = character(), freq = numeric())
        uniquewordsrow <- dataset[[rownum]] %>%
                str_extract_all(., boundary("word")) %>%
                unique()
        for(i in seq_along(uniquewordsrow)) {
                uniqueword <- paste("\\b", uniquewordsrow[i], "\\b", sep = "")
                uniquewordfreq <- str_count(dataset[rownum], uniqueword)
                rowfreq[i,1] <- uniquewordsrow[i]
                rowfreq[i,2] <- uniquewordfreq
        }
        return(rowfreq)
}
```


#### Multirow word frequency counter

Function that nested the previous function, within a loop along the length of the dataset.  The returned dataset (wordfrequency) was grouped by words, summarized and arranged by frequency in order to easily visualize the most frequent words within each dataset.

```{r}
wordsetfreq <- function(dataset) {
        setfreq <- data_frame(word = character(), freq = numeric())
        for(i in seq_along(dataset)) {
               setfreq <- bind_rows(setfreq, wordrowfreq(dataset, rownum = i))
        }
        setfreq <- setfreq %>%
                group_by(word) %>%
                summarize (., freq = sum(freq)) %>%
                arrange(., desc(freq))
        return(setfreq)
}
```

As an example follows an extract of the results obtained using rows 1 through 3 of news data:

```{r, echo = FALSE}
head(wordsetfreq(dataset = words_news[1:3]), 3)
```

#### Single row 2-grams generator

Function that returns a list of 3-grams per row.

```{r}
twogramrow <- function(dataset, rownum) {
        twograms <- data_frame(twograms = character())
        if(length(dataset[[rownum]]) < 2) {
                twograms[1,1] <- NA
        } else {
                qtwograms <- length(dataset[[rownum]])-1
                for(j in 1:qtwograms) {
                        twograms[j,1] <- paste(dataset[[rownum]][[j]], dataset[[rownum]][[j+1]], sep = " ")
                }
        }
        return(unique(twograms))
}
```

As an example follows an extract of the results obtained using 3rd row of news data:

```{r, echo = FALSE}
head(twogramrow(dataset = words_news, rownum = 3), 3)
```

#### Single row 2-grams frequency counter

Function that nested the previous function, within a loop along the length of the dataset.  The returned dataset (twogramsfreq) was grouped by two-grams, summarized and arranged by frequency in order to easily visualize the most frequent ones within each dataset.

```{r}
twogramrowfreq <- function(dataset, rownum) {
        rowfreq <- data_frame(word = character(), freq = numeric())
        uniquetwogramsrow <- twogramrow(dataset, rownum)
        stringrow <- dataset[[rownum]] %>%
                        paste(., collapse = " ")
        for(i in seq_along(uniquetwogramsrow[[1]])) {
                uniquetwogram <- paste("\\b", uniquetwogramsrow[i,1], "\\b", sep = "")
                uniquetwogramfreq <- str_count(stringrow, uniquetwogram)
                rowfreq[i,1] <- uniquetwogramsrow[i,1]
                rowfreq[i,2] <- uniquetwogramfreq
        }
        return(rowfreq)
}
```

As an example follows an extract of the results obtained using 3rd row of news data:

```{r, echo = FALSE}
head(twogramrowfreq(dataset = words_news, rownum= 3), 3)
```

#### Single row 3-grams generator

Function that returns a list of 3-grams per row.

```{r}
threegramrow <- function(dataset, rownum) {
        threegrams <- data_frame(threegrams = character())
        if(length(dataset[[rownum]]) < 3) {
                threegrams[1,1] <- NA
        } else {
                qthreegrams <- length(dataset[[rownum]])-2
                for(j in 1:qthreegrams) {
                        threegrams[j,1] <- paste(dataset[[rownum]][[j]], dataset[[rownum]][[j+1]], dataset[[rownum]][[j+2]], sep = " ")
                }
        }
        return(unique(threegrams))
}
```

As an example follows an extract of the results obtained using 3rd row of news data:

```{r, echo = FALSE}
head(threegramrow(dataset = words_news, rownum = 3), 3)
```

#### Single row 3-grams frequency counter

Function that nested the previous function, within a loop along the length of the dataset.  The returned dataset (twogramsfreq) was grouped by three-grams, summarized and arranged by frequency in order to easily visualize the most frequent ones within each dataset.

```{r}
threegramrowfreq <- function(dataset, rownum) {
        rowfreq <- data_frame(word = character(), freq = numeric())
        uniquethreegramsrow <- threegramrow(dataset, rownum)
        stringrow <- dataset[[rownum]] %>%
                        paste(., collapse = " ")
        for(i in seq_along(uniquethreegramsrow[[1]])) {
                uniquethreegram <- paste("\\b", uniquethreegramsrow[i,1], "\\b", sep = "")
                uniquethreegramfreq <- str_count(stringrow, uniquethreegram)
                rowfreq[i,1] <- uniquethreegramsrow[i,1]
                rowfreq[i,2] <- uniquethreegramfreq
        }
        return(rowfreq)
}
```

As an example follows an extract of the results obtained using 3rd row of news data:

```{r, echo = FALSE}
head(threegramrowfreq(dataset = words_news, rownum= 3), 3)
```

### II.b. Results Obtained by Dataset

When preparing the present preliminary analysis, word frequency was the only available indicator.  Following reports will include additional indicators. 

#### Twitter dataset

Once the word frequency function was applied to the first 1,000 rows of the dataset, the following result was obtained.

```{r, cache = TRUE}
freqword_twitter <- wordsetfreq(dataset = words_twitter[1:1000])
head(freqword_twitter)
```

```{r, echo = FALSE}
barplot(names.arg = head(freqword_twitter)$word, height = head(freqword_twitter)$freq)
```

#### Blogs dataset

Likewise, the following result was obtained.

```{r, cache = TRUE}
freqword_blogs <- wordsetfreq(dataset = words_blogs[1:1000])
head(freqword_blogs)
```

```{r, echo = FALSE}
barplot(names.arg = head(freqword_blogs)$word, height = head(freqword_blogs)$freq)
```

#### News dataset

Finally, the following result was obtained for the news dataset.

```{r, cache = TRUE}
freqword_news <- wordsetfreq(dataset = words_news[1:1000])
head(freqword_news)
```


```{r, echo = FALSE}
barplot(names.arg = head(freqword_news)$word, height = head(freqword_news)$freq)
```
