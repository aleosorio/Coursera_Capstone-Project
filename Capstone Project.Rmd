---
title: "Data Science Specialization Capstone Project"
author: "Alejandro Osorio"
date: "September 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set
library(tidyverse)
library(stringr)
library(cld3)
library(caret)
library(microbenchmark)
set.seed(123)
```

## Executive Summary

### Getting and Cleaning Data

The News dataset required cleaning in order to obtain 100% of the dataset.  The rest of the datasets (Twitter and Blogs) were obtained without cleaning required (details of the reading process, in Appendix I.a).  Details of the cleaning process required for the News dataset, in Appendix I.b.

Finally, the preliminary datasets containing separate-underscore-words were generated in Appendix I.c.

### Preliminary Analysis

Based on a word-frequency analysis, it's worth noting that words such as "the", "to", "a", "and" were the most frequent ones throughout all datasets.  Details of the quantitative and visual results, can be seen in Appendix II.b.

Additionally, some preliminary n-grams functions, that identify all two-grams and three-grams for each dataset row, were successfully programmed.  Details of said process in Appendix II.a.

## I. Appendix 1: Getting and Cleaning Data

### I.a. Reading Data

#### Twitter Dataset
```{r, cache = TRUE}
con <- file("en_US.twitter.txt", "r")
datatwitter <- readLines(con)
close(con)
```

Given the warning, the dimensions of both the obtained tibble and the original file, were compared.  They were equal.  Additionally, after some basic visual comparisons (head, tail, str, is.na), it was concluded that the obtained vector was ok to work with.

#### Blogs Dataset
```{r, cache = TRUE}
con <- file("en_US.blogs.txt", "r")
datablogs <- readLines(con)
close(con)
```

After the same basic visual comparisons (head, tail, str, is.na) used with the previous dataset, it was concluded that the obtained vector was also ok to work with.

#### News Dataset
```{r, cache = TRUE}
con <- file("en_US.news.txt", "r")
datanews_err <- readLines(con)
close(con)
```

Given the warning, the dimensions of both the obtained tibble and the original file, were compared.  The total number of lines obtained with the tibble (77,259) were much less than the ones from the original txt file (1,010,242).  Therefore, some cleaning was required, as follows.

### I.b. Cleaning Original Data

#### Cleaning News dataset

Further analysis to the "news" dataset, showed that line 77,259 was cut at the character "1", when compared to the one from the original file ("Ultra Edit" text editor was used for reading the original dataset, as well as the next cleaning steps). Therefore line 77,259 was analysed and an EOF character was found between texts "1" and "gallons", within the phrase "producing 1 -EOF- gallons".  Said character was deleted and the process re-run.  The result was a tibble with 766,277 rows (still less than the original file).  Another EOF character was found in row 766.277 (between texts "in" and "- inch", within the phrase "pears in EOF - inch slices."), deleted and the process re-run. Again, fewer rows were obtained (926,143).  Yet another EOF character was found in row 926,143 (between texts "in" and "-inch" within the phrase "Slice in EOF -inch-thick pieces"), deleted and the process re-run. Still, but last, fewer rows were obtained (948,564).  EOF character was found in row 926,143 (between texts "1" and "pounds" within the phrase "call for 1 EOF pounds of tomatoes"), deleted and the process re-run again.  Finally, 1,010,242 lines were obtained from the final fixed file, as follows:

```{r, cache = TRUE}
con <- file("en_US.news_FINAL.txt", "r")
datanews <- readLines(con)
close(con)
rm(con)
glimpse(datanews)
```

With this result, plus the same basic visual comparisons (head, tail, str, is.na) used with the previous datasets, it was concluded that the obtained vector was also ok to work with and therefore the cleaning stage was considered finished.

#### Splitting texts into pargraphs made up of lower-case-space-separated words 

The following function turns strings into lower-case format and splits them into pargraphs (using ".", ":", "?", "!" type characters and ommiting some trivial abbreviations, such as "sr.", "mr.", "mrs.", "apt.", "ave.", "blvd.", "bldg.", "ct.", "dr.", "ext.", "ft.", "fwy.", "hwy.", ln.", "mt.", "sq.", "rd.", "st.", "in.", "mi.", "yd.", "lb.", "oz.", "qt.", "pt.", "gal.", "cal.", "mph." ).  Each paragraph is treated as an extra vector element.  Additionally, all 1-gram, 2-gram and "" strings were eliminated.
```{r}
splitsentences <- function(dataset) {
        result <- str_to_lower(dataset) %>%
                str_split(., "(?<!((^|\\s)mr|(^|\\s)mrs|(^|\\s)sr|(^|\\s)apt|(^|\\s)ave|(^|\\s)blvd|(^|\\s)bldg|(^|\\s)ct|(^|\\s)dr|(^|\\s)ext|(^|\\s)ft|(^|\\s)fwy|(^|\\s)hwy|(^|\\s)ln|(^|\\s)mt|(^|\\s)sq|(^|\\s)st|(^|\\s)rd|(^|\\s)in|(^|\\s)mi|(^|\\s)yd|(^|\\s)lb|(^|\\s)oz|(^|\\s)qt|(^|\\s)pt|(^|\\s)gal|(^|\\s)cal|(^|\\s)mph))\\.|\\?|\\!|\\:") %>%
                unlist(., use.names = FALSE) %>%
                subset(., str_count(., pattern = boundary("word")) > 2)
        return(result)
}
```

Applying the function to original datasets:
```{r, cache = TRUE}
words_twitter <- splitsentences(datatwitter)
words_blogs <- splitsentences(datablogs)
words_news <- splitsentences(datanews)
```

Finally, the following function converts the previous one's output into a vector with the strings turned into one-space-separated words:
```{r}
tidystrings <- function(dataset) {
        datalength <- length(dataset)
        result <- dataset %>%
                str_extract_all(., boundary("word"))
        for(i in 1:datalength) {
                result[[i]] <- str_flatten(result[[i]], collapse = " ")
        }
        return(unlist(result, use.names = FALSE))
}
```

Applying the function to create the final datasets:
```{r, cache = TRUE}
words_twitter <- tidystrings(words_twitter)
words_blogs <- tidystrings(words_blogs)
words_news <- tidystrings(words_news)
```

As an example of the separate-underscore-words results obtained, the following string:
```{r, echo = FALSE}
datanews[2]
```

was turned into:
```{r, echo = FALSE}
words_news[2:4]
```

#### Cleaning foreign languages/ damaged strings

Some rows show words weren't useful for predictive modeling, such as:
```{r}
words_news[15]
```

Eventhough it's english written, the extra work needed to clean such cases was considered unnecesary.  Instead, a language detector (library "cld3") was used to filter such cases.  As an example:
```{r}
detect_language(words_news[15])
```

The result "bg-Latn" doesn't match the expected one: "en" (which stands for english).  Therefore, all three datasets were filtered from rows other than "en", using the following function:
```{r, cache = TRUE}
langfilt <- function(lng, dataset) {
        lngdetectvect <- detect_language(dataset)
        lngvect <- lng == lngdetectvect %>%
                        replace_na(., FALSE)
        dataset <- dataset[lngvect == TRUE]
        return(dataset)
}
```

Obtained datasets:
```{r, cache = TRUE}
final_blogs <- langfilt(lng = "en", dataset = words_blogs)
final_news <- langfilt(lng = "en", dataset = words_news)
final_twitter <- langfilt(lng = "en", dataset = words_twitter)
```

Finally, after obtaining some results from further analysis, in spite of the previous language filtering, strings with the "â" character were found rather frequently, such as:
```{r, cache = TRUE}
final_news[head(unlist(str_which(final_news, "â"), use.names = FALSE), 5)]
```

Additionally, the highest % of such strings within the obtained datasets was as follows:
```{r, cache = TRUE}
length(final_blogs[unlist(str_which(final_blogs, "â"), use.names = FALSE)])/ length(final_blogs)
```

Therefore, a final extra cleaning was carried on, eliminating strings that contained said character, as follows:
```{r, cache = TRUE}
final_blogs <- final_blogs[-unlist(str_which(final_blogs, "â"), use.names = FALSE)]
final_news <- final_news[-unlist(str_which(final_news, "â"), use.names = FALSE)]
final_twitter <- final_twitter[-unlist(str_which(final_twitter, "â"), use.names = FALSE)]
```

## II. Appendix 2: Preliminary Analysis

### II.a. Basic Analysis Functions

#### Single row n-grams generator

Function that inputs a vector of character strings and, with one specific character string, returns another character vector with all of its possible n-grams.

```{r}
ngramrow <- function(ngram, dataset, rownum) {
        ngrams <- character(length = 0)
        lengthstring <- str_count(dataset[rownum], boundary("word"))
        if(lengthstring < ngram) {
                ngrams <- NA
        } else {
                qngrams <- lengthstring+1-ngram
                for(j in 1:qngrams) {
                                ngrams <- c(ngrams, word(dataset[rownum], j, ngram + j - 1))
                                }
                }
        return(unique(ngrams))
        }
```

As an example follows a 3-gram generation, using the 3rd row of the news data:

3-gram:
```{r, cache = TRUE, echo = FALSE}
head(ngramrow(ngram = 3, dataset = final_news, rownum = 3), 5)
```

#### Single row n-grams frequency counter

Function that nested the previous function, within a loop along the length of the selected vector's string.  The returned data frame was grouped by the corresponding n-grams, summarized and arranged by frequency in order to easily visualize the most frequent ones in the given row.
```{r}
ngramrowfreq <- function(ngram, dataset, rownum) {
        stringrow <- dataset[rownum]
        uniquengramsrow <- ngramrow(ngram, dataset, rownum)
        numngrams <- length(uniquengramsrow)
        rowfreq <- data_frame(string = character(), freq = numeric())
        for(i in 1:numngrams) {
                uniquengram <- paste("\\b", uniquengramsrow[i], "\\b", sep = "")
                uniquengramfreq <- str_count(stringrow, uniquengram)
                rowfreq[i,1] <- uniquengramsrow[i]
                rowfreq[i,2] <- uniquengramfreq
        }
        return(rowfreq)
}
```

As an example follows a 3-gram row frequency count, using the 3rd row of the news data:
```{r, cache = TRUE, echo = FALSE}
head(ngramrowfreq(ngram = 3, dataset = final_news, rownum= 3), 5)
```

#### Multirow n-grams frequency counter

Function that nested the previous function, within a loop along the length of the dataset.  The returned data frame was grouped by n-gram, summarized and arranged by frequency.  Additionally, it added a column with the dimension of each n-gram.

```{r}
ngramsetfreq <- function(ngram, dataset) {
        setfreq <- data_frame(string = character(), freq = numeric())
        for(i in 1:length(dataset)) {
               setfreq <- bind_rows(setfreq, ngramrowfreq(ngram, dataset, rownum = i))
        }
        setfreq <- setfreq %>%
                group_by(string) %>%
                summarize (., freq = sum(freq)) %>%
                arrange(., desc(freq)) %>%
                mutate(., n = ngram)
        return(setfreq)
}
```

As an example follows a 3-gram frequency count, using the first 5 rows of the news dataset:
```{r, cache = TRUE, echo = FALSE}
head(ngramsetfreq(ngram = 3, dataset = final_news[1:5]), 4)
```

### II.b. Results Obtained by Dataset

#### Twitter dataset

Applying the Multirow n-grams frequency counter to the first 1,000 rows of the dataset, the following results were obtained for 1-grams:

1-grams:
```{r, echo = FALSE, cache = TRUE}
freq_twitter <- ngramsetfreq(ngram = 1, dataset = final_twitter[1:1000])
barplot(names.arg = head(freq_twitter)$string, height = head(freq_twitter)$freq)
```

#### Blogs dataset

Likewise, the following 3-gram results were obtained for this dataset.
```{r, echo = FALSE, cache = TRUE}
freq_blogs <- ngramsetfreq(ngram = 3, dataset = final_blogs[1:1000])
barplot(names.arg = head(freq_blogs)$string, height = head(freq_blogs)$freq)
```

#### News dataset

Finally, the following results were obtained for the news dataset.
```{r, echo = FALSE, cache = TRUE}
freq_news <- ngramsetfreq(ngram = 3, dataset = final_news[1:1000])
barplot(names.arg = head(freq_news)$string, height = head(freq_news)$freq)
```

## III. Appendix 3: Predictive Models

### III.a. Predictive Algorithms Based on Searching Within Pre-calculated n-gram Datasets

#### Preliminary Data Set

The training set was built by mixing a sample of each one of the three datasets, as follows:
```{r, cache = TRUE}
sampletrain <- c(sample(final_blogs, 1000), sample(final_news, 1000), sample(final_twitter, 1000))
```

#### n-gram Sets

Function that generates n-grams (from 2-grams up to the n specified) based on the previously obtained Preliminary Data Set. 
```{r}
ngramsetgen <- function(n, dataset) {
        ngramsets <- data_frame(string = character(), freq = integer(), n = integer())
        for(i in 2:n) {
                ngramset <- ngramsetfreq(ngram = i, dataset = dataset)
                ngramsets <- bind_rows(ngramsets, ngramset)
        }
        return(ngramsets)
}
```

The following set was pre-calculated, in order to run the predictive algorithm as an n-gram search machine.

```{r, cache = TRUE}
samplengram <- ngramsetgen(5, sampletrain)
```

#### Function that, given a string, filters from the sample ngram dataset the n-grams from which to predict the next word

In case there is no n-gram available in the samplengram dataset, it reduces n by 1 (and the string in 1 word) and repeats until it finds the best available n-gram set.
```{r}
ngrampred <- function(string, dataset) {
        numwords <- str_count(string, boundary("word"))
        disp <- string %in% dataset[[1]]
        while(disp == FALSE) {
                numwords <- numwords - 1
                string <- word(string, 2, -1)
                disp <- string %in% dataset[[1]]
        }
        stringsearch <- paste("^", string, "\\b", sep = "")
        ngramset <- dataset %>%
                filter(., n == (numwords + 1) & str_detect(string, stringsearch))
        return(ngramset)
}
```

As an example, if applied with the 2-gram "on the":
```{r, cache = TRUE}
head(ngrampred(string = "on the", dataset = samplengram), 3)
```

Likewise, if applied with the 4-gram "and a case of":
```{r, cache = TRUE}
head(ngrampred(string = "and a case of", dataset = samplengram), 3)
```
It can be seen that, as there were no 5-grams including the string "and a case of <word>", the function then reduced the search string to "a case of" and then searched for a 4-gram that included said string ("a case of <word>"), and so forth until it found the solutions.

#### Function that selects the predicted word, from previous function's output

In case several rows have the same maximum frequency, a random row within that subset was chosen, as follows:
```{r}
wordpred <- function(string, dataset) {
        word <- ngrampred(string, dataset) %>%
                filter(., freq == max(freq)) %>%
                sample_n(., 1) %>%
                select(., string) %>%
                word(., -1)
        return(word)
}
```

As an example, if applied again with the 4-gram string "and a case of":
```{r, cache = TRUE}
wordpred(string = "and a case of", dataset = samplengram)
```

The problem with the "search engine" as a standalone approach, is that it requires enormous amount of processing in order to generate all n-gram sets from which to predict later.  Therefore a different approach was attempted further on, consisting on generating the n-gram sets on demand, based on the string that requires prediction, as follows in the next chapter.

### III.c. Predictive Algorithms Based on On Demand n-gram Dataset Generation and Search

#### Function that generates dataset to work with
```{r}
datagen <- function(percent, dataset1, dataset2, dataset3) {
        dim1 <- ceiling(length(dataset1) * percent)
        dim2 <- ceiling(length(dataset2) * percent)
        dim3 <- ceiling(length(dataset3) * percent)
        datafinal <- c(sample(dataset1, dim1), sample(dataset2, dim2), sample(dataset3, dim3))
        return(datafinal)
}
```

The testing dataset was built with 40% of each available datasets (blogs, news and twitter), as follows:
```{r, cache = TRUE}
testdata <- datagen(.6, final_blogs, final_news, final_twitter)
```

The following (pretty large) dataset was obtained:
```{r}
str(testdata)
```


#### Function that extracts specified string, plus its next word, from a row
```{r}
stringrow <- function(string, dataset, numrow) {
        matches <- str_extract_all(dataset[numrow], paste("(?<=(^|\\s))", string, "\\s\\w+", sep = ""))[[1]]
        return(matches)
}
```

As an example, if applied with the string "plans" to the 4rth row of the news dataset:
```{r, cache = TRUE}
stringrow("plans", final_news, numrow = 4)
```

#### Function that nests previous function and applies it to a dataset
```{r}
stringset <- function(string, dataset) {
        datlength <- length(dataset)
        matches <- c()
        for(i in 1:datlength) {
                stringmatch <- stringrow(string, dataset, numrow = i)
                matches <- c(matches, stringmatch)
        }
        matches <- data_frame(string = matches) %>%
                mutate(., freq = 1) %>%
                group_by(string) %>%
                summarize (., freq = sum(freq)) %>%
                arrange(., desc(freq))
        return(matches)
}
```

As an example, if applied with the string "to the" to a small sample of the testdata dataset:
```{r, cache = TRUE}
head(stringset("to the", sample(testdata, 5000)), 5)
```

The performance obtained with the later functions, was extremly better than the previous one.  Much so that the whole news dataset took less than 40 seconds to complete.

#### Wrapper that applies stringset function only to strings available in the input set

It starts by converting the input string to the same testing dataset format (lower_case_space_separated_no_punctuation_words).  In case there's no match for the string in the dataset, it continues by reducing the string's n-gram by 1 (by taking out its first word) and searching until it finds the first available string. If it goes down to a 1-gram string and finds no result, it returns an empty dataframe (with the same format of a non-empty output).
```{r}
stringsure <- function(string, dataset) {
        strsearch <- str_to_lower(string) %>%
                str_extract_all(., boundary("word")) %>%
                unlist(., use.names = FALSE) %>%
                str_flatten(., collapse = " ")
        strwhere <- paste("(?<=(^|\\s))", strsearch, "\\s\\w+", sep = "") %>%
                str_which(dataset, .)
        strdisp <- length(strwhere)
        while(strdisp == 0 & str_count(strsearch, boundary("word")) > 1) {
                strsearch <- word(strsearch, 2, -1)
                strwhere <- paste("(?<=(^|\\s))", strsearch, "\\s\\w+", sep = "") %>%
                        str_which(dataset, .)
                strdisp <- length(strwhere)
        }
        if(length(strwhere) > 0) {
                result <- stringset(strsearch , dataset[strwhere])
        } else {
                result <- data_frame(string = character(), freq = numeric())
        }
        return(result)
}
```

As an example, if applied with the string "the faith during the" on the testing dataset:
```{r, cache = TRUE}
head(stringsure(string = "the faith during the", dataset = testdata), 5)
```

It's worth mentioning that the function took less than 15 seconds to search within the whole testdata dataset, which contains around MM3.5 strings.

#### Function that selects the predicted word, from previous function's output

Same previous function, wordpred, was modified in order to work with the current stringsure function, as follows:
```{r}
wordpred2 <- function(string, dataset) {
        pred <- stringsure(string, dataset)
        if(nrow(pred) >0) {
                pred <- filter(pred, freq == max(freq)) %>%
                        sample_n(., 1) %>%
                        select(., string) %>%
                        word(., -1)
        } else {
                pred <- "NOT ENOUGH DATA AVAILABLE TO PREDICT. TRY AGAIN WITH A DIFFERENT STRING"
        }
        return(pred)
}
```

As an example, if applied again with the 4-gram string "the faith during the":
```{r, cache = TRUE}
wordpred2(string = "the faith during the", dataset = testdata)
```

## IV. Appendix 4: Final Solution

### IV.a. Popular N-grams

#### Required Functions

##### Function that returns a vector of unique values, from an sorted numeric vector (decreasing frequencies in this case), that represent a certain % of its total unique values
```{r}
varselect <- function(inputvect, cutperc) {
        uniquevect <- unique(inputvect) %>%
                sort(., decreasing = TRUE)
        numunique <- uniquevect %>%
                length(.) %>%
                "*"(cutperc) %>%
                ceiling(.)
        return(uniquevect[1:numunique])
}
```

##### Function that subsets a "string/freq/n"" type dataframe, within the freq that corresponds to the output (row number) given by the varselect function
If the lowest freq in "freqvect" (vector of selected frequencies in the function) is equal to 1 (therefore capturing the whole DF), it filters the DF with frequencies > 1.
```{r}
stringselect <- function(inputdf, cutperc) {
        freqvect <- varselect(inputdf$freq, cutperc)
        result <- filter(inputdf, inputdf$freq %in% freqvect)
        return(result)
}
```

As an example, if applied to the 1-gram frequency twitter dataset previously obtained (in II.b.), with a 80% threshold, the following "tail" was obtained:
```{r, cache = TRUE}
tail(stringselect(freq_twitter, .8))
```

Meaning that all 1-grams with equal or higher frequency than 13, that belong in the freq_twitter$string vector, would be considered within said threshold.

##### Function that generates a matrix with n predicted next-words to a string vector, within a sample of testdata dataset
```{r}
mtxwordpred <- function(stringvect, numpred, datatest, sampdata) {
        lengthvect <- length(stringvect)
        sampdata <- ceiling(length(datatest)*sampdata)
        finresult <- matrix(nrow = lengthvect, ncol = numpred)
        for(j in 1:numpred) {
                findata <- sample(datatest, sampdata)
                result <- c()
                for (i in 1:lengthvect) {
                        result <- c(result, wordpred2(stringvect[i], findata))
                }
                finresult[,j] <- result
        }
        return(finresult)
}
```

As an example, the results obtained with a subset of freq_twitter$string as a 1-gram input vector, were the following:

1. Input string vector:
```{r, cache = TRUE}
freq_twitter$string[1:10]
```

2. Output obtained, with 8 predicted next words, within a dataset 1% of testdata:
```{r, cache = TRUE}
mtxwordpred(freq_twitter$string[1:10], 8, testdata, 0.001)
```

So, with 10 iterations, pretty stable predictions were obtained.

##### Function that identifies the most frequent value within a row of a matrix
```{r}
freqvals <- function(x) {
        ux <- unique(x)
        tab <- match(x,ux) %>%
                tabulate(.)
        ux[tab == max(tab)]
}
```

As an example, when applied to a sring vector (it can return one or more solutions):
```{r, cache = TRUE}
freqvals(c("hello", "bye", "one", "hello", "bye", "two"))
```


##### Wrapper that converts input and output of mtxwordpred function into a DF with input-string vector, word-predicted vector and solutions-per-string vector
Criteria for selecting each predicted word consists on its frequency within the n predicted next words ("numpred"" parameter in vectwordpred function).  When freqvals returns more than one solution, it randomly selects one (through sampling).
```{r}
vectwordpred <- function(stringvect, numpred, datatest, sampdata) {
        mtxpred <- mtxwordpred(stringvect, numpred, datatest, sampdata)
        modrows <- apply(mtxpred, 1, freqvals) %>%
                sapply(., sample, size = 1)
        uniqrows <- apply(mtxpred, 1, n_distinct)
        result <- data_frame(string = stringvect, unique = uniqrows, modes = modrows)
        return(result)
}
```

As an example, when applied to the previously used subset of freq_twitter$string (with 5 predicted next words, within a dataset 0.1% of testdata):
```{r, cache = TRUE}
vectwordpred(freq_twitter$string[1:10], 10, testdata, 0.001)
```

This time, the "a" string obtains 5 possible solutions, out of which "little" was selected (either by frequency or by randomly selecting one out of all that share the same frequency).  Therefore, a higher "sampdata" value might be the solution for a more representative result.

#### Obtained Results

##### 1-Grams
Selected 1-grams dataset, using 20.000 strings out of the testdata dataset
```{r, cache = TRUE}
popular1grams <- ngramsetfreq(ngram = 1, dataset = sample(testdata, 20000))
```

Final 1-grams dataset, with an accumulated frequency threshold (cutperc) of 90%:
```{r, cache = TRUE}
popular1grams_final <- stringselect(popular1grams, .9)
```

Obtained input dataset:
```{r}
glimpse(popular1grams_final)
```

Applying vectwordpred function to the obtained input, with 10 predicted next words, within a dataset 0.5% of testdata (5x the sample used previously):
```{r, cache = TRUE}
wordpred1grams <- vectwordpred(popular1grams_final$string, 10, testdata, 0.005)
```

The following resulting DF was obtained:
```{r}
head(wordpred1grams, 5)
```

##### 2-grams
Likewise:
```{r, cache = TRUE}
popular2grams <- ngramsetfreq(ngram = 2, dataset = sample(testdata, 20000))
```

```{r, cache = TRUE}
popular2grams_final <- stringselect(popular2grams, .9)
```

Due to fastest processing time, % size of the testdata was raised to 1%:
```{r, cache = TRUE}
wordpred2grams <- vectwordpred(popular2grams_final$string, 10, testdata, 0.01)
```

##### 3-grams
Applying same parameters used with 2-grams:
```{r, cache = TRUE}
popular3grams <- ngramsetfreq(ngram = 3, dataset = sample(testdata, 20000))
```

```{r, cache = TRUE}
popular3grams_final <- stringselect(popular3grams, .9)
```

```{r, cache = TRUE}
wordpred3grams <- vectwordpred(popular3grams_final$string, 10, testdata, 0.01)
```

##### 4-grams
Likewise:
```{r, cache = TRUE}
popular4grams <- ngramsetfreq(ngram = 4, dataset = sample(testdata, 20000))
```

```{r, cache = TRUE}
popular4grams_final <- stringselect(popular4grams, .9)
```

```{r, cache = TRUE}
wordpred4grams <- vectwordpred(popular4grams_final$string, 10, testdata, 0.01)
```

##### 5-grams
```{r, cache = TRUE}
popular5grams <- ngramsetfreq(ngram = 5, dataset = sample(testdata, 20000))
```

% cut for the freq variable, was lowered down to 75% in order to obtain all results with freq >= 2. The obtained length reflects the fact that beyond 4grams, pre-processed datasets' added value goes down quickly:
```{r, cache = TRUE}
popular5grams_final <- stringselect(popular5grams, .75)
```

```{r, cache = TRUE}
wordpred5grams <- vectwordpred(popular5grams_final$string, 10, testdata, 0.01)
```

##### Final popular n-grams dataset
Binding "popularNgrams_final" tables:
```{r, echo = TRUE}
wordpredgrams <- bind_rows(wordpred1grams, wordpred2grams, wordpred3grams, wordpred4grams, wordpred5grams)
```

Further fine tunning of the pre-calculated n-gram dataset, could be based on:

1. Modifying the size of each dataset when applying stringselect function (currently between at 90%).
2. Modifying the % of testdata ("sampdata" parameter) when applying the vectwordpred function (currently between 0.5 and 1%).
3. Imposing a minimum "unique" value (50% of the numpred value, for example) in wordpredNgrams datasets (cases with "unique" = 10 when "numpred" = 10 means a total random result that may as well not be elegible for pre-calculated predictions).

Major considerations: processing time vs precision.

### IV.b. Mixed and Final Solution

#### stringsure function's upgrade into a mixed search criteria
This function was stripped from the initial input string's format check (transfered to the wordpredfinal function) and added an extra search criteria, consisting on always conducting the next search, after an n-gram reduction, whithin the pre-calculated n-grams dataset (ngramdata = wordpredgrams).  If unsuccessful, it applies the original stringsure's search criteria within the corresponding dataset (stringdata = sample of testdata), and so on.  Therefore it absorbed wordpred2's functionalities.

```{r}
stringsurefinal <- function(string, ngramdata, stringdata) {
        strlength <- str_count(string, boundary("word"))
        strsearch <- paste("(?<=(^|\\s))", string, "\\s\\w+", sep = "")
        strwhere <- str_which(stringdata, strsearch)
        strdisp <- length(strwhere)
        ngramwhere <- integer(0)
        while(strdisp == 0 & strlength > 1) {
                string <- word(string, 2, -1)
                strlength <- str_count(string, boundary("word"))
                ngramwhere <- which(string == ngramdata$string)
                if(length(ngramwhere) > 0) {
                        ngramresult <- ngramdata$modes[ngramwhere]
                        strdisp <- ngramwhere
                } else {
                        strsearch <- paste("(?<=(^|\\s))", string, "\\s\\w+", sep = "")
                        strwhere <- str_which(stringdata, strsearch)
                        strdisp <- length(strwhere)
                }
        }
        if(strdisp > 0) {
                if(length(ngramwhere) >0) {
                        result <- ngramresult
                } else {
                        result <- stringset(string , stringdata[strwhere]) %>%
                                filter(., freq == max(freq)) %>%
                                sample_n(., 1) %>%
                                select(., string) %>%
                                word(., -1)
                }
        } else {
                result <- "NOT ENOUGH DATA AVAILABLE TO PREDICT. TRY AGAIN WITH A DIFFERENT STRING"
        }
        return(result)
}
```


#### Function that returns the predicted word, with an initial search within the pre-calculated popular n-grams dataset (wordpredgrams)
It starts by carrying on the input string's format check (converting the input string into the same wordpredfinal$string format) and reducing its ngram down to "ngramlength" (5 ended up being the number, beyond which very few succesfull searches were obtained) . If there's no direct match within the pre-calculated n-grams dataset, all additional predictions are conducted through the stringsurefinal function, within a testdata sample (defined by the "precision" parameter).

```{r}
wordpredfinal <- function(string, precision, ngramdata, stringdata) {
        ngramlength <- 5
        strsearch <- str_to_lower(string) %>%
                str_extract_all(., boundary("word")) %>%
                unlist(., use.names = FALSE) %>%
                str_flatten(., collapse = " ") %>%
                if_else(str_count(., boundary("word")) > ngramlength, word(., str_count(., boundary("word")) - ngramlength + 1, -1), .)
        strlength <- str_count(strsearch, boundary("word"))
        ngramwhere <- which(strsearch == ngramdata$string)
        if(length(ngramwhere) > 0) {
                result <- ngramdata$modes[ngramwhere]
        } else {
                samplength <- ceiling(length(testdata) * precision)
                stringdata <- sample(stringdata, samplength)
                result <- stringsurefinal(strsearch, ngramdata, stringdata)
        }
        return(result)
}
```

### IV.c. Performance Testing

#### Case 1: String available in ngram pre-calculated dataset
String to use was selected from the last values of said dataset:
```{r, cache = TRUE, echo = TRUE}
tail(wordpredgrams)
```

Using the microbenchmark library, the test was conducted using a longer string, based on the last value of the previous table.  Additionally the "precision" parameter was set up to the values 0.1, 0.15 and 0.2 (10%, 15% and 20% of the testdata dataset).  Length of testdata dataset (number of strings to search from), associated to the previous values, were more or less: 352k, 528k and 704k.  Setting the number of iterations to 5, the following results were obtained:
```{r, cache = TRUE, echo = TRUE}
microbenchmark(wordpredfinal("the best you treat yourself, the more energy you", .1, wordpredgrams, testdata), wordpredfinal("the best you treat yourself the more energy you", .15, wordpredgrams, testdata), wordpredfinal("the best you treat yourself the more energy you", .2, wordpredgrams, testdata), times = 5)
```

So when dealing with strings available in the pre-calculated ngram dataset, the size of the "precision" parameter, within the interval 0.1 - 0.2, regarding the median processing time, is irrelevant (the result was always obtained in a small fraction of a second).

#### Case 2: String not available in ngram pre-calculated dataset
First, a word not available in the dataset, was selected to create the testing string.
```{r}
"specifically" %in% wordpredgrams$string
```

Therefore the string "i would like to know, specifically" was used, applying the same previous parameters, plus an extra case with "precision" set to 5% (176k strings to search from the testdata dataset) as follows:
```{r, cache = TRUE, echo = TRUE}
microbenchmark(wordpredfinal("i would like to know, specifically", .05, wordpredgrams, testdata), wordpredfinal("i would like to know, specifically", .1, wordpredgrams, testdata), wordpredfinal("i would like to know, specifically", .15, wordpredgrams, testdata), wordpredfinal("i would like to know, specifically", .2, wordpredgrams, testdata), times = 5)
```

Interestingly enough, a pretty linear response was obtained, starting with less than a second for a 5% "precision" parameter, up to more than 3 seconds for a 20% value.  Considering the response time as a critical performance variable, and imposing its max value to be close to 1 second, it was decided to set the "precision" parameter to 0.075 (that meant securing 264k testdata strings to the analysis, which is more than 10 times the value used to determine the most popular ngrams -20000-).